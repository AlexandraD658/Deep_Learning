{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\kaggle\\input\\deepfake-detection-jph\\facenet_pytorch-2.0.1-py3-none-any.whl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Requirement '/kaggle/input/deepfake-detection-jph/facenet_pytorch-2.0.1-py3-none-any.whl' looks like a filename, but the file does not exist\n",
      "ERROR: Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: 'C:\\\\kaggle\\\\input\\\\deepfake-detection-jph\\\\facenet_pytorch-2.0.1-py3-none-any.whl'\n",
      "\n",
      "'cp' n'est pas reconnu en tant que commande interne\n",
      "ou externe, un programme ex‚cutable ou un fichier de commandes.\n"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/deepfake-detection-jph/facenet_pytorch-2.0.1-py3-none-any.whl\n",
    "!cp -r /kaggle/input/pretrainedmodels /kaggle/working/; cd /kaggle/working/pretrainedmodels/pretrained-models.pytorch ; pip install .\n",
    "#!pip install /kaggle/input/deepfake-detection-jph/efficientnet_pytorch-0.6.1/efficientnet_pytorch-0.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pickle\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import skimage.measure\n",
    "import albumentations as A\n",
    "from tqdm.notebook import tqdm \n",
    "from albumentations.pytorch import ToTensor \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision.models.video import mc3_18, r2plus1d_18\n",
    "\n",
    "from facenet_pytorch import MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "\n",
    "INPUT_DIR = \"/kaggle/input/deepfake-detection-challenge/test_videos\"\n",
    "PRETRAINED_MODELS_3D = [{'type':'i3d',\n",
    "                         'path':\"/kaggle/input/deepfake-detection-jph/j3d_e1_l0.1374.model\"},\n",
    "                        {'type':'res34',\n",
    "                         'path':\"/kaggle/input/deepfake-detection-jph/res34_1cy_minaug_nonorm_e4_l0.1794.model\"},\n",
    "                        {'type':'mc3_112',\n",
    "                         'path':\"/kaggle/input/deepfake-detection-jph/mc3_18_112_1cy_lilaug_nonorm_e9_l0.1905.model\"},\n",
    "                        {'type':'mc3_224',\n",
    "                         'path':\"/kaggle/input/deepfake-detection-jph/mc3_18_112t224_1cy_lilaug_nonorm_e7_l0.1901.model\"},\n",
    "                        {'type':'r2p1_112',\n",
    "                         'path':\"/kaggle/input/deepfake-detection-jph/r2p1_18_8_112tr_112te_e12_l0.1741.model\"},\n",
    "                        {'type':'i3d',\n",
    "                         'path':\"/kaggle/input/deepfake-detection-jph/i3dcutmix_e11_l0.1612.model\"},\n",
    "                        {'type':'r2p1_112',\n",
    "                         'path':\"/kaggle/input/deepfake-detection-jph/r2plus1dcutmix_112_e10_l0.1608.model\"}]\n",
    "\n",
    "# Face detection\n",
    "MAX_FRAMES_TO_LOAD = 100\n",
    "MIN_FRAMES_FOR_FACE = 30\n",
    "MAX_FRAMES_FOR_FACE = 100\n",
    "FACE_FRAMES = 10\n",
    "MAX_FACES_HIGHTHRESH = 5\n",
    "MAX_FACES_LOWTHRESH = 1\n",
    "FACEDETECTION_DOWNSAMPLE = 0.25\n",
    "MTCNN_THRESHOLDS = (0.8, 0.8, 0.9)  # Default [0.6, 0.7, 0.7]\n",
    "MTCNN_THRESHOLDS_RETRY = (0.5, 0.5, 0.5)\n",
    "MMTNN_FACTOR = 0.71  # Default 0.709 p\n",
    "TWO_FRAME_OVERLAP = False\n",
    "\n",
    "# Inference\n",
    "PROB_MIN, PROB_MAX = 0.001, 0.999\n",
    "REVERSE_PROBS = True\n",
    "DEFAULT_MISSING_PRED = 0.5\n",
    "USE_FACE_FUNCTION = np.mean\n",
    "\n",
    "# 3D inference\n",
    "RATIO_3D = 1\n",
    "OUTPUT_FACE_SIZE = (256, 256)\n",
    "PRE_INFERENCE_CROP = (224, 224)\n",
    "\n",
    "# 2D\n",
    "RATIO_2D = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video(filename, every_n_frames=None, specific_frames=None, to_rgb=True, rescale=None, inc_pil=False, max_frames=None):\n",
    "    \"\"\"Loads a video.\n",
    "    Called by:\n",
    "    \n",
    "    1) The finding faces algorithm where it pulls a frame every FACE_FRAMES frames up to MAX_FRAMES_TO_LOAD at a scale of FACEDETECTION_DOWNSAMPLE, and then half that if there's a CUDA memory error.\n",
    "    \n",
    "    2) The inference loop where it pulls EVERY frame up to a certain amount which it the last needed frame for each face for that video\"\"\"\n",
    "    \n",
    "    assert every_n_frames or specific_frames, \"Must supply either every n_frames or specific_frames\"\n",
    "    assert bool(every_n_frames) != bool(specific_frames), \"Supply either 'every_n_frames' or 'specific_frames', not both\"\n",
    "    \n",
    "    cap = cv2.VideoCapture(filename)\n",
    "    n_frames_in = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    width_in = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height_in = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    if rescale:\n",
    "        rescale = rescale * 1920./np.max((width_in, height_in))\n",
    "    \n",
    "    width_out = int(width_in*rescale) if rescale else width_in\n",
    "    height_out = int(height_in*rescale) if rescale else height_in\n",
    "    \n",
    "    if max_frames:\n",
    "        n_frames_in = min(n_frames_in, max_frames)\n",
    "    \n",
    "    if every_n_frames:\n",
    "        specific_frames = list(range(0,n_frames_in,every_n_frames))\n",
    "    \n",
    "    n_frames_out = len(specific_frames)\n",
    "    \n",
    "    out_pil = []\n",
    "\n",
    "    out_video = np.empty((n_frames_out, height_out, width_out, 3), np.dtype('uint8'))\n",
    "\n",
    "    i_frame_in = 0\n",
    "    i_frame_out = 0\n",
    "    ret = True\n",
    "\n",
    "    while (i_frame_in < n_frames_in and ret):\n",
    "        \n",
    "        try:\n",
    "            try:\n",
    "        \n",
    "                if every_n_frames == 1:\n",
    "                    ret, frame_in = cap.read()  # Faster if reading all frames\n",
    "                else:\n",
    "                    ret = cap.grab()\n",
    "\n",
    "                    if i_frame_in not in specific_frames:\n",
    "                        i_frame_in += 1\n",
    "                        continue\n",
    "\n",
    "                    ret, frame_in = cap.retrieve()\n",
    "                    \n",
    "#                 print(f\"Reading frame {i_frame_in}\")\n",
    "\n",
    "                if rescale:\n",
    "                    frame_in = cv2.resize(frame_in, (width_out, height_out))\n",
    "                if to_rgb:\n",
    "                    frame_in = cv2.cvtColor(frame_in, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error for frame {i_frame_in} for video {filename}: {e}; using 0s\")\n",
    "                frame_in = np.zeros((height_out, width_out, 3))\n",
    "\n",
    "        \n",
    "            out_video[i_frame_out] = frame_in\n",
    "            i_frame_out += 1\n",
    "\n",
    "            if inc_pil:\n",
    "                try:  # https://www.kaggle.com/zaharch/public-test-errors\n",
    "                    pil_img = Image.fromarray(frame_in)\n",
    "                except Exception as e:\n",
    "                    print(f\"Using a blank frame for video {filename} frame {i_frame_in} as error {e}\")\n",
    "                    pil_img = Image.fromarray(np.zeros((224,224,3), dtype=np.uint8))  # Use a blank frame\n",
    "                out_pil.append(pil_img)\n",
    "\n",
    "            i_frame_in += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error for file {filename}: {e}\")\n",
    "\n",
    "    cap.release()\n",
    "    \n",
    "    if inc_pil:\n",
    "        return out_video, out_pil, rescale\n",
    "    else:\n",
    "        return out_video, rescale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the three Regions Of Interest (ROI) of a frame :\n",
    "<img src=\"https://www.mathworks.com/help/visionhdl/ref/roi_selection_ex2.png\" align=\"left\"> </img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roi_for_each_face(faces_by_frame, probs, video_shape, temporal_upsample, upsample=1):\n",
    "    \n",
    "    \"\"\"\n",
    "        Remove color channels\n",
    "        Replace blank frames if face(s) in neighbouring frames with overlap\n",
    "        Detect all faces in through time \n",
    "        Return those faces, and for each face, all the corresponding ROIs present in the each input frame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create boolean face array\n",
    "    frames_video, rows_video, cols_video, channels_video = video_shape\n",
    "    frames_video = math.ceil(frames_video)\n",
    "    boolean_face_3d = np.zeros((frames_video, rows_video, cols_video), dtype=np.bool)  # Remove colour channel\n",
    "    proba_face_3d = np.zeros((frames_video, rows_video, cols_video)).astype('float32')\n",
    "    for i_frame, faces in enumerate(faces_by_frame):\n",
    "        if faces is not None:  # May not be a face in the frame\n",
    "            for i_face, face in enumerate(faces):\n",
    "                left, top, right, bottom = face\n",
    "                boolean_face_3d[i_frame, int(top):int(bottom), int(left):int(right)] = True\n",
    "                proba_face_3d[i_frame, int(top):int(bottom), int(left):int(right)] = probs[i_frame][i_face]\n",
    "                \n",
    "    # Replace blank frames if face(s) in neighbouring frames with overlap\n",
    "    for i_frame, frame in enumerate(boolean_face_3d):\n",
    "        if i_frame == 0 or i_frame == frames_video-1:  # Can't do this for 1st or last frame\n",
    "            continue\n",
    "        if True not in frame:\n",
    "            if TWO_FRAME_OVERLAP:\n",
    "                if i_frame > 1:\n",
    "                    pre_overlap = boolean_face_3d[i_frame-1] | boolean_face_3d[i_frame-2]\n",
    "                else:\n",
    "                    pre_overlap = boolean_face_3d[i_frame-1]\n",
    "                if i_frame < frames_video-2:\n",
    "                    post_overlap = boolean_face_3d[i_frame+1] | boolean_face_3d[i_frame+2]\n",
    "                else:\n",
    "                    post_overlap = boolean_face_3d[i_frame+1]\n",
    "                neighbour_overlap = pre_overlap & post_overlap\n",
    "            else:\n",
    "                neighbour_overlap = boolean_face_3d[i_frame-1] & boolean_face_3d[i_frame+1]\n",
    "            boolean_face_3d[i_frame] = neighbour_overlap\n",
    "\n",
    "    # Find faces through time\n",
    "    id_face_3d, n_faces = skimage.measure.label(boolean_face_3d, return_num=True)\n",
    "    region_labels, counts = np.unique(id_face_3d, return_counts=True)\n",
    "    # Get rid of background=0\n",
    "    region_labels, counts = region_labels[1:], counts[1:]\n",
    "    ###################\n",
    "    # DESCENDING SIZE #\n",
    "    ###################\n",
    "    descending_size = np.argsort(counts)[::-1]\n",
    "    labels_by_size = region_labels[descending_size]\n",
    "    ####################\n",
    "    # DESCENDING PROBS #\n",
    "    ####################\n",
    "    probs = [np.mean(proba_face_3d[id_face_3d == i_face]) for i_face in region_labels]\n",
    "    descending_probs = np.argsort(probs)[::-1]\n",
    "    labels_by_probs = region_labels[descending_probs]\n",
    "    # Iterate over faces in video\n",
    "    rois = [] ; face_maps = []\n",
    "    for i_face in labels_by_probs:#labels_by_size:\n",
    "        # Find the first and last frame containing the face\n",
    "        frames = np.where(np.any(id_face_3d == i_face, axis=(1, 2)) == True)\n",
    "        starting_frame, ending_frame = frames[0].min(), frames[0].max()\n",
    "\n",
    "        # Iterate over the frames with faces in and find the min/max cols/rows (bounding box)\n",
    "        cols, rows = [], []\n",
    "        for i_frame in range(starting_frame, ending_frame + 1):\n",
    "            rs = np.where(np.any(id_face_3d[i_frame] == i_face, axis=1) == True)\n",
    "            rows.append((rs[0].min(), rs[0].max()))\n",
    "            cs = np.where(np.any(id_face_3d[i_frame] == i_face, axis=0) == True)\n",
    "            cols.append((cs[0].min(), cs[0].max()))\n",
    "        frame_from, frame_to = starting_frame*temporal_upsample, ((ending_frame+1)*temporal_upsample)-1\n",
    "        rows_from, rows_to = np.array(rows)[:, 0].min(), np.array(rows)[:, 1].max()\n",
    "        cols_from, cols_to = np.array(cols)[:, 0].min(), np.array(cols)[:, 1].max()\n",
    "        \n",
    "        frame_to = min(frame_to, frame_from + MAX_FRAMES_FOR_FACE)\n",
    "        \n",
    "        if frame_to - frame_from >= MIN_FRAMES_FOR_FACE:\n",
    "            tmp_face_map = id_face_3d.copy()\n",
    "            tmp_face_map[tmp_face_map != i_face] = 0\n",
    "            tmp_face_map[tmp_face_map == i_face] = 1\n",
    "            face_maps.append(tmp_face_map[frame_from//temporal_upsample:frame_to//temporal_upsample+1])\n",
    "            rois.append(((frame_from, frame_to),\n",
    "                         (int(rows_from*upsample), int(rows_to*upsample)),\n",
    "                         (int(cols_from*upsample), int(cols_to*upsample))))\n",
    "            \n",
    "    return np.array(rois), face_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[1, 1, 0, 0, 0],\n",
       "         [1, 1, 0, 2, 2],\n",
       "         [0, 0, 0, 2, 2],\n",
       "         [0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0],\n",
       "         [3, 3, 0, 2, 2],\n",
       "         [3, 3, 0, 2, 2]]], dtype=int64),\n",
       " 3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([\n",
    "    [\n",
    "        [1, 1, 0, 0, 0],\n",
    "        [1, 1, 0, 1, 1],\n",
    "        [0, 0, 0, 1, 1],\n",
    "        [0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0]\n",
    "    ], \n",
    "    [\n",
    "        [0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 1, 1],\n",
    "        [1, 1, 0, 1, 1]\n",
    "    ]\n",
    "])\n",
    "a, b = skimage.measure.label(x, return_num= True)\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3], dtype=int64), array([34,  4,  8,  4], dtype=int64))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(a, return_counts= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation\n",
    "<img src=\"https://compression.ru/video/frame_rate_conversion/images/N_Conversion.gif\" align=\"left\"> </img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_center(bbox):\n",
    "    \"\"\"\n",
    "        bbox (bounding box) is the box delimited by the rows x1 and x2, and the columns y1 and y2 \n",
    "        This function return the center of the bbox ((x1+x2)/2 , (y1+y2)/2)\n",
    "    \"\"\"\n",
    "    \n",
    "    x1, y1, x2, y2 = bbox\n",
    "    return (x1+x2)/2, (y1+y2)/2\n",
    "\n",
    "\n",
    "def get_coords(faces_roi):\n",
    "    \"\"\"\n",
    "        Returns the coordinates of the first and last elements of \"faces_roi\" whose values are 1\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    coords = np.argwhere(faces_roi == 1)      # getting all indices of the array elements whose values are 1 (in this context, we get all coordinates of black pixels) \n",
    "    #print(coords)\n",
    "    if coords.shape[0] == 0:\n",
    "        return None\n",
    "    \n",
    "    y1, x1 = coords[0]                        # first element\n",
    "    y2, x2 = coords[-1]                       # last element\n",
    "    \n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "\n",
    "def interpolate_center(c1, c2, length):\n",
    "    \n",
    "    \"\"\"\n",
    "        We interpolate c1, c2 linearly. The returned element is an array of shape (2, length) \n",
    "        \n",
    "        Eg: interpolate_center((2, 4), (3, 9), 5) -->  [[2, 2.2, 2.4, 2.6, 2.8, 3] , [4, 5, 6, 7, 8, 9]]\n",
    "        \n",
    "        All columns of the returned array are our centers. We have now {length} centers\n",
    "    \"\"\"\n",
    "    x1, y1 = c1\n",
    "    x2, y2 = c2\n",
    "    xi, yi = np.linspace(x1, x2, length), np.linspace(y1, y2, length)\n",
    "    return np.vstack([xi, yi]).transpose(1,0)\n",
    "\n",
    "\n",
    "def get_faces(faces_roi, upsample): \n",
    "    \"\"\"\n",
    "        faces_roi[i][j] = roi of the face i in the frame j\n",
    "        If returned face is done by his bounding box represented by  (x1, x2, y1, y2)\n",
    "        x1, x2 are the horizontal delimiters and y1, y1 are the vertical delimiters\n",
    "        \n",
    "        This function :\n",
    "            1 - Compute the size, called \"max_dim\", to assign to all boxes to return\n",
    "            2 - Calculate the centers of all the rois\n",
    "            3 - Interpolate all the centers. The new centers are the centers of the boxes to return \n",
    "            4 - Build the boxes through the centers and the max_dim, and translating the boxes that fall outside of frame\n",
    "            5 - return the faces\n",
    "    \"\"\"\n",
    "    all_faces = []                            \n",
    "    rows = faces_roi[0].shape[1]              \n",
    "    cols = faces_roi[0].shape[2]\n",
    "    \n",
    "    \n",
    "    for i in range(len(faces_roi)):                                                                      \n",
    "        faces = np.asarray([get_coords(faces_roi[i][j]) for j in range(len(faces_roi[i]))]) \n",
    "        if faces[0] is None:  faces[0] = faces[1]                                                   \n",
    "        if faces[-1] is None: faces[-1] = faces[-2]\n",
    "        if None in faces:\n",
    "            #print(faces)\n",
    "            raise Exception('This should not have happened ...')\n",
    "        all_faces.append(faces)     \n",
    "        \n",
    "    extracted_faces = []\n",
    "    for face in all_faces:          \n",
    "        # Get max dim size\n",
    "        max_dim = np.concatenate([face[:,2]-face[:,0],face[:,3]-face[:,1]])\n",
    "        max_dim = np.percentile(max_dim, 90)\n",
    "        # Enlarge by 1.2\n",
    "        max_dim = int(max_dim * 1.2)\n",
    "        # Get center coords\n",
    "        centers = np.asarray([get_center(_) for _ in face])\n",
    "        # Interpolate\n",
    "        centers = np.vstack([interpolate_center(centers[i], centers[i+1], length=10) for i in range(len(centers)-1)]).astype('int')\n",
    "        x1y1 = centers - max_dim // 2\n",
    "        x2y2 = centers + max_dim // 2 \n",
    "        x1, y1 = x1y1[:,0], x1y1[:,1]\n",
    "        x2, y2 = x2y2[:,0], x2y2[:,1]\n",
    "        # If x1 or y1 is negative, turn it to 0\n",
    "        # Then add to x2 y2 or y2\n",
    "        x2[x1 < 0] -= x1[x1 < 0]\n",
    "        y2[y1 < 0] -= y1[y1 < 0]\n",
    "        x1[x1 < 0] = 0\n",
    "        y1[y1 < 0] = 0\n",
    "        # If x2 or y2 is too big, turn it to max image shape\n",
    "        # Then subtract from y1\n",
    "        y1[y2 > rows] += rows - y2[y2 > rows]\n",
    "        x1[x2 > cols] += cols - x2[x2 > cols]\n",
    "        y2[y2 > rows] = rows\n",
    "        x2[x2 > cols] = cols\n",
    "        vidface = np.asarray([[x1[_],y1[_],x2[_],y2[_]] for _,c in enumerate(centers)]) \n",
    "        vidface = (vidface*upsample).astype('int')\n",
    "        extracted_faces.append(vidface)\n",
    "\n",
    "    return extracted_faces\n",
    "\n",
    "def detect_face_with_mtcnn(mtcnn_model, pil_frames, facedetection_upsample, video_shape, face_frames):\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    \"\"\"\n",
    "    boxes, _probs = mtcnn_model.detect(pil_frames, landmarks=False)\n",
    "    faces, faces_roi = get_roi_for_each_face(faces_by_frame=boxes, probs=_probs, video_shape=video_shape, temporal_upsample=face_frames, upsample=facedetection_upsample)\n",
    "    coords = [] if len(faces_roi) == 0 else get_faces(faces_roi, upsample=facedetection_upsample)\n",
    "    return faces, coords\n",
    "\n",
    "def face_detection_wrapper(mtcnn_model, videopath, every_n_frames, facedetection_downsample, max_frames_to_load):\n",
    "    \n",
    "    \"\"\"\n",
    "        Extract faces directly from the videopath.\n",
    "        1 - load_video\n",
    "        2 - detect_face_with_mtcnn\n",
    "        3 - extract the faces present on the video and all their coordinates on each frame of the video\n",
    "        \n",
    "        if no face is extracted, downsample by half\n",
    "        if no face is still extacted, upsample twice\n",
    "    \"\"\"\n",
    "    video, pil_frames, rescale = load_video(videopath, every_n_frames=every_n_frames, to_rgb=True, rescale=facedetection_downsample, inc_pil=True, max_frames=max_frames_to_load)\n",
    "    if len(pil_frames):\n",
    "        try:\n",
    "            faces, coords = detect_face_with_mtcnn(mtcnn_model=mtcnn, \n",
    "                                                   pil_frames=pil_frames, \n",
    "                                                   facedetection_upsample=1/rescale, \n",
    "                                                   video_shape=video.shape, \n",
    "                                                   face_frames=every_n_frames)\n",
    "        except RuntimeError:  # Out of CUDA RAM\n",
    "            print(f\"Failed to process {videopath} ! Downsampling x2 ...\")\n",
    "            video, pil_frames, rescale = load_video(videopath, every_n_frames=every_n_frames, to_rgb=True, rescale=facedetection_downsample/2, inc_pil=True, max_frames=max_frames_to_load)\n",
    "\n",
    "            try:\n",
    "                faces, coords = detect_face_with_mtcnn(mtcnn_model=mtcnn, \n",
    "                                       pil_frames=pil_frames, \n",
    "                                       facedetection_upsample=1/rescale, \n",
    "                                       video_shape=video.shape, \n",
    "                                       face_frames=every_n_frames)\n",
    "            except RuntimeError:\n",
    "                print(f\"Failed on downsample ! Skipping...\")\n",
    "                return [], []\n",
    "                \n",
    "    else:\n",
    "        print('Failed to fetch frames ! Skipping ...')\n",
    "        return [], []\n",
    "        \n",
    "    if len(faces) == 0:\n",
    "        print('Failed to find faces ! Upsampling x2 ...')\n",
    "        try:\n",
    "            video, pil_frames, rescale = load_video(videopath, every_n_frames=every_n_frames, to_rgb=True, rescale=facedetection_downsample*2, inc_pil=True, max_frames=max_frames_to_load)\n",
    "            faces, coords = detect_face_with_mtcnn(mtcnn_model=mtcnn, \n",
    "                                                   pil_frames=pil_frames, \n",
    "                                                   facedetection_upsample=1/rescale, \n",
    "                                                   video_shape=video.shape, \n",
    "                                                   face_frames=every_n_frames)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return [], []\n",
    "    \n",
    "    return faces, coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 videos !\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2299e1fed4b34179a1d040d1332d2af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found faces for 1 videos; 0 missing\n"
     ]
    }
   ],
   "source": [
    "videopaths = sorted(glob(os.path.join(INPUT_DIR, \"*.mp4\")))\n",
    "\n",
    "# videopaths = [\"C:/Users/elton/Videos/Captures/(1) Idriss Aberkane sans filtre [EN DIRECT] - YouTube - Google Chrome 2020-09-30 16-10-34.mp4\"]\n",
    "mtcnn = MTCNN(margin=0, keep_all=True, post_process=False, select_largest=False, thresholds=MTCNN_THRESHOLDS, factor=MMTNN_FACTOR)\n",
    "\n",
    "print(f'Found {len(videopaths)} videos !')\n",
    "\n",
    "mtcnn = MTCNN(margin=0, keep_all=True, post_process=False, select_largest=False, device='cuda:0', thresholds=MTCNN_THRESHOLDS, factor=MMTNN_FACTOR)\n",
    "\n",
    "faces_by_videopath = {}\n",
    "coords_by_videopath = {}\n",
    "\n",
    "for i_video, videopath in enumerate(tqdm(videopaths)):\n",
    "    faces, coords = face_detection_wrapper(mtcnn, videopath, every_n_frames=FACE_FRAMES, facedetection_downsample=FACEDETECTION_DOWNSAMPLE, max_frames_to_load=MAX_FRAMES_TO_LOAD)\n",
    "    \n",
    "    if len(faces):\n",
    "        faces_by_videopath[videopath]  = faces[:MAX_FACES_HIGHTHRESH]\n",
    "        coords_by_videopath[videopath] = coords[:MAX_FACES_HIGHTHRESH]\n",
    "    else:\n",
    "        print(f\"Found no faces for {videopath} !\")\n",
    "\n",
    "        \n",
    "del mtcnn101\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "videopaths_missing_faces = {p for p in videopaths if p not in faces_by_videopath}\n",
    "print(f\"Found faces for {len(faces_by_videopath)} videos; {len(videopaths_missing_faces)} missing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1247.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n",
      "(1, 1)\n",
      "(2, 2)\n",
      "(3, 3)\n",
      "(4, 4)\n",
      "(5, 5)\n",
      "(6, 6)\n",
      "(7, 7)\n",
      "(8, 8)\n",
      "(9, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in enumerate(tqdm.tqdm(range(10))):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-8a0d95324897>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmtcnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMTCNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmargin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_all\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpost_process\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselect_largest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cuda:0'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMTCNN_THRESHOLDS_RETRY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMMTNN_FACTOR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi_video\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvideopath\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideopaths_missing_faces\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     faces, coords = face_detection_wrapper(mtcnn, \n\u001b[0;32m      5\u001b[0m                                            \u001b[0mvideopath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, image_size, margin, min_face_size, thresholds, factor, post_process, select_largest, selection_method, keep_all, device)\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselection_method\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    610\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 612\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m     def register_backward_hook(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    379\u001b[0m                 \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m                     \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    608\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mconvert_to_format\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[1;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;31m# are found or any other error occurs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m         \u001b[1;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;31m# we need to just return without initializing in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "mtcnn = MTCNN(margin=0, keep_all=True, post_process=False, select_largest=False ,device='cuda:0', thresholds=MTCNN_THRESHOLDS_RETRY, factor=MMTNN_FACTOR)\n",
    "\n",
    "for i_video, videopath in enumerate(tqdm(videopaths_missing_faces)):\n",
    "    faces, coords = face_detection_wrapper(mtcnn, \n",
    "                                           videopath, \n",
    "                                           every_n_frames=FACE_FRAMES, \n",
    "                                           facedetection_downsample=FACEDETECTION_DOWNSAMPLE, \n",
    "                                           max_frames_to_load=MAX_FRAMES_TO_LOAD)\n",
    "            \n",
    "    if len(faces):\n",
    "        faces_by_videopath[videopath]  = faces[:MAX_FACES_LOWTHRESH]\n",
    "        coords_by_videopath[videopath] = coords[:MAX_FACES_LOWTHRESH]\n",
    "    else:\n",
    "        print(f\"Found no faces for {videopath} !\")\n",
    "\n",
    "del mtcnn\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "faces_by_videopath = dict(sorted(faces_by_videopath.items(), key=lambda x: x[0]))\n",
    "videopaths_missing_faces = {p for p in videopaths if p not in faces_by_videopath}\n",
    "print(f\"Found faces for {len(faces_by_videopath)} videos; {len(videopaths_missing_faces)} missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_by_videopath = dict(sorted(faces_by_videopath.items(), key=lambda x: x[0]))\n",
    "videopaths_missing_faces = {p for p in videopaths if p not in faces_by_videopath}\n",
    "print(f\"Found faces for {len(faces_by_videopath)} videos; {len(videopaths_missing_faces)} missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Let's build our InceptionI3D model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-d7dad6eebb00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mMaxPool3dSamePadding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaxPool3d\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_pad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \"\"\"\n\u001b[0;32m      5\u001b[0m             \u001b[0mCompute\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpadding\u001b[0m \u001b[0mneeded\u001b[0m \u001b[0mat\u001b[0m \u001b[0man\u001b[0m \u001b[0minput\u001b[0m \u001b[0msignal\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;36m3\u001b[0m\u001b[0mD\u001b[0m \u001b[0mmax\u001b[0m \u001b[0mpooling\u001b[0m \u001b[0mover\u001b[0m \u001b[0mthis\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class MaxPool3dSamePadding(nn.MaxPool3d):\n",
    "\n",
    "    def compute_pad(self, dim, s):\n",
    "        \"\"\"\n",
    "            Compute the padding needed at an input signal for a 3D max pooling over this input\n",
    "        \"\"\"\n",
    "        if s % self.stride[dim] == 0:\n",
    "            return max(self.kernel_size[dim] - self.stride[dim], 0)\n",
    "        else:\n",
    "            return max(self.kernel_size[dim] - (s % self.stride[dim]), 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # compute 'same' padding\n",
    "        (batch, channel, t, h, w) = x.size()\n",
    "        # print t,h,w\n",
    "        out_t = np.ceil(float(t) / float(self.stride[0]))\n",
    "        out_h = np.ceil(float(h) / float(self.stride[1]))\n",
    "        out_w = np.ceil(float(w) / float(self.stride[2]))\n",
    "        # print out_t, out_h, out_w\n",
    "        pad_t = self.compute_pad(0, t)\n",
    "        pad_h = self.compute_pad(1, h)\n",
    "        pad_w = self.compute_pad(2, w)\n",
    "        # print pad_t, pad_h, pad_w\n",
    "\n",
    "        pad_t_f = pad_t // 2\n",
    "        pad_t_b = pad_t - pad_t_f\n",
    "        pad_h_f = pad_h // 2\n",
    "        pad_h_b = pad_h - pad_h_f\n",
    "        pad_w_f = pad_w // 2\n",
    "        pad_w_b = pad_w - pad_w_f\n",
    "\n",
    "        pad = (pad_w_f, pad_w_b, pad_h_f, pad_h_b, pad_t_f, pad_t_b)\n",
    "        # print x.size()\n",
    "        # print pad\n",
    "        x = F.pad(x, pad)\n",
    "        return super(MaxPool3dSamePadding, self).forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-d84f80182f2e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mUnit3D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     def __init__(self, in_channels,\n\u001b[0;32m      4\u001b[0m                  \u001b[0moutput_channels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                  \u001b[0mkernel_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Unit3D(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels,\n",
    "                 output_channels,\n",
    "                 kernel_shape=(1, 1, 1),\n",
    "                 stride=(1, 1, 1),\n",
    "                 padding=0,\n",
    "                 activation_fn=F.relu,\n",
    "                 use_batch_norm=True,\n",
    "                 use_bias=False,\n",
    "                 name='unit_3d'):\n",
    "\n",
    "        \"\"\"Initializes Unit3D module.\"\"\"\n",
    "        \n",
    "        super(Unit3D, self).__init__()\n",
    "\n",
    "        self._output_channels = output_channels\n",
    "        self._kernel_shape = kernel_shape\n",
    "        self._stride = stride\n",
    "        self._use_batch_norm = use_batch_norm\n",
    "        self._activation_fn = activation_fn\n",
    "        self._use_bias = use_bias\n",
    "        self.name = name\n",
    "        self.padding = padding\n",
    "\n",
    "        self.conv3d = nn.Conv3d(in_channels=in_channels,\n",
    "                                out_channels=self._output_channels,\n",
    "                                kernel_size=self._kernel_shape,\n",
    "                                stride=self._stride,\n",
    "                                padding=0,\n",
    "                                # we always want padding to be 0 here. We will dynamically pad based on input size in forward function\n",
    "                                bias=self._use_bias)\n",
    "\n",
    "        if self._use_batch_norm:\n",
    "            self.bn = nn.BatchNorm3d(self._output_channels, eps=0.001, momentum=0.01)\n",
    "\n",
    "    def compute_pad(self, dim, s):\n",
    "        if s % self._stride[dim] == 0:\n",
    "            return max(self._kernel_shape[dim] - self._stride[dim], 0)\n",
    "        else:\n",
    "            return max(self._kernel_shape[dim] - (s % self._stride[dim]), 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # compute 'same' padding\n",
    "        (batch, channel, t, h, w) = x.size()\n",
    "        # print t,h,w\n",
    "        out_t = np.ceil(float(t) / float(self._stride[0]))\n",
    "        out_h = np.ceil(float(h) / float(self._stride[1]))\n",
    "        out_w = np.ceil(float(w) / float(self._stride[2]))\n",
    "        # print out_t, out_h, out_w\n",
    "        pad_t = self.compute_pad(0, t)\n",
    "        pad_h = self.compute_pad(1, h)\n",
    "        pad_w = self.compute_pad(2, w)\n",
    "        # print pad_t, pad_h, pad_w\n",
    "\n",
    "        pad_t_f = pad_t // 2\n",
    "        pad_t_b = pad_t - pad_t_f\n",
    "        pad_h_f = pad_h // 2\n",
    "        pad_h_b = pad_h - pad_h_f\n",
    "        pad_w_f = pad_w // 2\n",
    "        pad_w_b = pad_w - pad_w_f\n",
    "\n",
    "        pad = (pad_w_f, pad_w_b, pad_h_f, pad_h_b, pad_t_f, pad_t_b)\n",
    "        # print x.size()\n",
    "        # print pad\n",
    "        x = F.pad(x, pad)\n",
    "        # print x.size()\n",
    "\n",
    "        x = self.conv3d(x)\n",
    "        if self._use_batch_norm:\n",
    "            x = self.bn(x)\n",
    "        if self._activation_fn is not None:\n",
    "            x = self._activation_fn(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Inception Module for 3D tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img\n",
    "src='https://www.researchgate.net/profile/Xinshuo_Weng/publication/337425041/figure/fig1/AS:827714932072449@1574354026721/3D-inception-module.png'\n",
    "align = 'left'> \n",
    "\n",
    "</img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, name):\n",
    "        super(InceptionModule, self).__init__()\n",
    "\n",
    "        self.b0 = Unit3D(in_channels=in_channels, output_channels=out_channels[0], kernel_shape=[1, 1, 1], padding=0,\n",
    "                         name=name + '/Branch_0/Conv3d_0a_1x1')\n",
    "        self.b1a = Unit3D(in_channels=in_channels, output_channels=out_channels[1], kernel_shape=[1, 1, 1], padding=0,\n",
    "                          name=name + '/Branch_1/Conv3d_0a_1x1')\n",
    "        self.b1b = Unit3D(in_channels=out_channels[1], output_channels=out_channels[2], kernel_shape=[3, 3, 3],\n",
    "                          name=name + '/Branch_1/Conv3d_0b_3x3')\n",
    "        self.b2a = Unit3D(in_channels=in_channels, output_channels=out_channels[3], kernel_shape=[1, 1, 1], padding=0,\n",
    "                          name=name + '/Branch_2/Conv3d_0a_1x1')\n",
    "        self.b2b = Unit3D(in_channels=out_channels[3], output_channels=out_channels[4], kernel_shape=[3, 3, 3],\n",
    "                          name=name + '/Branch_2/Conv3d_0b_3x3')\n",
    "        self.b3a = MaxPool3dSamePadding(kernel_size=[3, 3, 3],\n",
    "                                        stride=(1, 1, 1), padding=0)\n",
    "        self.b3b = Unit3D(in_channels=in_channels, output_channels=out_channels[5], kernel_shape=[1, 1, 1], padding=0,\n",
    "                          name=name + '/Branch_3/Conv3d_0b_1x1')\n",
    "        self.name = name\n",
    "\n",
    "    def forward(self, x):\n",
    "        b0 = self.b0(x)\n",
    "        b1 = self.b1b(self.b1a(x))\n",
    "        b2 = self.b2b(self.b2a(x))\n",
    "        b3 = self.b3b(self.b3a(x))\n",
    "        return torch.cat([b0, b1, b2, b3], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The general Inception Model architechture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"https://media.geeksforgeeks.org/wp-content/uploads/20200429201549/Inceptionv1_architecture.png\"> </img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionI3d(nn.Module):\n",
    "    \"\"\"Inception-v1 I3D architecture.\n",
    "    The model is introduced in:\n",
    "        Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset\n",
    "        Joao Carreira, Andrew Zisserman\n",
    "        https://arxiv.org/pdf/1705.07750v1.pdf.\n",
    "    See also the Inception architecture, introduced in:\n",
    "        Going deeper with convolutions\n",
    "        Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\n",
    "        Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich.\n",
    "        http://arxiv.org/pdf/1409.4842v1.pdf.\n",
    "    \"\"\"\n",
    "\n",
    "    # Endpoints of the model in order. During construction, all the endpoints up\n",
    "    # to a designated `final_endpoint` are returned in a dictionary as the\n",
    "    # second return value.\n",
    "    VALID_ENDPOINTS = (\n",
    "        'Conv3d_1a_7x7',\n",
    "        'MaxPool3d_2a_3x3',\n",
    "        'Conv3d_2b_1x1',\n",
    "        'Conv3d_2c_3x3',\n",
    "        'MaxPool3d_3a_3x3',\n",
    "        'Mixed_3b',\n",
    "        'Mixed_3c',\n",
    "        'MaxPool3d_4a_3x3',\n",
    "        'Mixed_4b',\n",
    "        'Mixed_4c',\n",
    "        'Mixed_4d',\n",
    "        'Mixed_4e',\n",
    "        'Mixed_4f',\n",
    "        'MaxPool3d_5a_2x2',\n",
    "        'Mixed_5b',\n",
    "        'Mixed_5c',\n",
    "        'Logits',\n",
    "        'Predictions',\n",
    "    )\n",
    "\n",
    "    def __init__(self, num_classes=400, spatial_squeeze=True, output_method='per_frame',\n",
    "                 final_endpoint='Logits', name='inception_i3d', in_channels=3, dropout_keep_prob=0.5):\n",
    "        \"\"\"Initializes I3D model instance.\n",
    "        Args:\n",
    "          num_classes: The number of outputs in the logit layer (default 400, which\n",
    "              matches the Kinetics dataset).\n",
    "          spatial_squeeze: Whether to squeeze the spatial dimensions for the logits\n",
    "              before returning (default True).\n",
    "          final_endpoint: The model contains many possible endpoints.\n",
    "              `final_endpoint` specifies the last endpoint for the model to be built\n",
    "              up to. In addition to the output at `final_endpoint`, all the outputs\n",
    "              at endpoints up to `final_endpoint` will also be returned, in a\n",
    "              dictionary. `final_endpoint` must be one of\n",
    "              InceptionI3d.VALID_ENDPOINTS (default 'Logits').\n",
    "          name: A string (optional). The name of this module.\n",
    "        Raises:\n",
    "          ValueError: if `final_endpoint` is not recognized.\n",
    "        \"\"\"\n",
    "\n",
    "        if final_endpoint not in self.VALID_ENDPOINTS:\n",
    "            raise ValueError('Unknown final endpoint %s' % final_endpoint)\n",
    "\n",
    "        super(InceptionI3d, self).__init__()\n",
    "        self.output_method = output_method\n",
    "        assert output_method in ('per_frame', 'avg_pool', 'max_pool', 'dual_pool')\n",
    "        self._num_classes = num_classes\n",
    "        self._spatial_squeeze = spatial_squeeze\n",
    "        self._final_endpoint = final_endpoint\n",
    "        self.logits = None\n",
    "\n",
    "        if self._final_endpoint not in self.VALID_ENDPOINTS:\n",
    "            raise ValueError('Unknown final endpoint %s' % self._final_endpoint)\n",
    "\n",
    "        self.end_points = {}\n",
    "        end_point = 'Conv3d_1a_7x7'\n",
    "        self.end_points[end_point] = Unit3D(in_channels=in_channels, output_channels=64, kernel_shape=[7, 7, 7],\n",
    "                                            stride=(2, 2, 2), padding=(3, 3, 3), name=name + end_point)\n",
    "        if self._final_endpoint == end_point: return\n",
    "\n",
    "        end_point = 'MaxPool3d_2a_3x3'\n",
    "        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[1, 3, 3], stride=(1, 2, 2),\n",
    "                                                          padding=0)\n",
    "        if self._final_endpoint == end_point: return\n",
    "\n",
    "        end_point = 'Conv3d_2b_1x1'\n",
    "        self.end_points[end_point] = Unit3D(in_channels=64, output_channels=64, kernel_shape=[1, 1, 1], padding=0,\n",
    "                                            name=name + end_point)\n",
    "        if self._final_endpoint == end_point: return\n",
    "\n",
    "        end_point = 'Conv3d_2c_3x3'\n",
    "        self.end_points[end_point] = Unit3D(in_channels=64, output_channels=192, kernel_shape=[3, 3, 3], padding=1,\n",
    "                                            name=name + end_point)\n",
    "        if self._final_endpoint == end_point: return\n",
    "\n",
    "        end_point = 'MaxPool3d_3a_3x3'\n",
    "        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[1, 3, 3], stride=(1, 2, 2),\n",
    "                                                          padding=0)\n",
    "        if self._final_endpoint == end_point: return\n",
    "\n",
    "        end_point = 'Mixed_3b'\n",
    "        self.end_points[end_point] = InceptionModule(192, [64, 96, 128, 16, 32, 32], name + end_point)\n",
    "        if self._final_endpoint == end_point: return\n",
    "\n",
    "        end_point = 'Mixed_3c'\n",
    "        self.end_points[end_point] = InceptionModule(256, [128, 128, 192, 32, 96, 64], name + end_point)\n",
    "        if self._final_endpoint == end_point: return\n",
    "\n",
    "        end_point = 'MaxPool3d_4a_3x3'\n",
    "        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[3, 3, 3], stride=(2, 2, 2),\n",
    "                                                          padding=0)\n",
    "        if self._final_endpoint == end_point: return\n",
    "\n",
    "        end_point = 'Mixed_4b'\n",
    "        self.end_points[end_point] = InceptionModule(128 + 192 + 96 + 64, [192, 96, 208, 16, 48, 64], name + end_point)\n",
    "        if self._final_endpoint == end_point: return\n",
    "\n",
    "        end_point = 'Mixed_4c'\n",
    "        self.end_points[end_point] = InceptionModule(192 + 208 + 48 + 64, [160, 112, 224, 24, 64, 64], name + end_point)\n",
    "        if self._final_endpoint == end_point: return\n",
    "\n",
    "        end_point = 'Mixed_4d'\n",
    "        self.end_points[end_point] = InceptionModule(160 + 224 + 64 + 64, [128, 128, 256, 24, 64, 64], name + end_point)\n",
    "        if self._final_endpoint == end_point: return\n",
    "\n",
    "        end_point = 'Mixed_4e'\n",
    "        self.end_points[end_point] = InceptionModule(128 + 256 + 64 + 64, [112, 144, 288, 32, 64, 64], name + end_point)\n",
    "        if self._final_endpoint == end_point: return\n",
    "\n",
    "        end_point = 'Mixed_4f'\n",
    "        self.end_points[end_point] = InceptionModule(112 + 288 + 64 + 64, [256, 160, 320, 32, 128, 128],\n",
    "                                                     name + end_point)\n",
    "        if self._final_endpoint == end_point: return\n",
    "\n",
    "        end_point = 'MaxPool3d_5a_2x2'\n",
    "        self.end_points[end_point] = MaxPool3dSamePadding(kernel_size=[2, 2, 2], stride=(2, 2, 2),\n",
    "                                                          padding=0)\n",
    "        if self._final_endpoint == end_point: return\n",
    "\n",
    "        end_point = 'Mixed_5b'\n",
    "        self.end_points[end_point] = InceptionModule(256 + 320 + 128 + 128, [256, 160, 320, 32, 128, 128],\n",
    "                                                     name + end_point)\n",
    "        if self._final_endpoint == end_point: return\n",
    "\n",
    "        end_point = 'Mixed_5c'\n",
    "        self.end_points[end_point] = InceptionModule(256 + 320 + 128 + 128, [384, 192, 384, 48, 128, 128],\n",
    "                                                     name + end_point)\n",
    "        if self._final_endpoint == end_point: return\n",
    "\n",
    "        end_point = 'Logits'\n",
    "        self.avg_pool = nn.AvgPool3d(kernel_size=[2, 7, 7],\n",
    "                                     stride=(1, 1, 1))\n",
    "        self.dropout = nn.Dropout(dropout_keep_prob)\n",
    "        self.logits = Unit3D(in_channels=384 + 384 + 128 + 128, output_channels=self._num_classes,\n",
    "                             kernel_shape=[1, 1, 1],\n",
    "                             padding=0,\n",
    "                             activation_fn=None,\n",
    "                             use_batch_norm=False,\n",
    "                             use_bias=True,\n",
    "                             name='logits')\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def replace_logits(self, num_classes):\n",
    "        self._num_classes = num_classes\n",
    "        self.logits = Unit3D(in_channels=384 + 384 + 128 + 128, output_channels=self._num_classes,\n",
    "                             kernel_shape=[1, 1, 1],\n",
    "                             padding=0,\n",
    "                             activation_fn=None,\n",
    "                             use_batch_norm=False,\n",
    "                             use_bias=True,\n",
    "                             name='logits')\n",
    "\n",
    "    def build(self):\n",
    "        for k in self.end_points.keys():\n",
    "            self.add_module(k, self.end_points[k])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for end_point in self.VALID_ENDPOINTS:\n",
    "            if end_point in self.end_points:\n",
    "                x = self._modules[end_point](x)  # use _modules to work with dataparallel\n",
    "\n",
    "        x = self.logits(self.dropout(self.avg_pool(x)))\n",
    "        if self._spatial_squeeze:\n",
    "            logits = x.squeeze(3).squeeze(3)\n",
    "        # logits is batch X time X classes, which is what we want to work with\n",
    "        if self.output_method == 'per_frame':\n",
    "            return F.interpolate(logits, 64, mode='linear')  # -> batch_size * 2 * 64\n",
    "        elif self.output_method == 'avg_pool':\n",
    "            avg_all_frames = F.adaptive_avg_pool1d(logits, output_size=1)\n",
    "            return avg_all_frames.squeeze(-1)  # -> batch_size * 2\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        for end_point in self.VALID_ENDPOINTS:\n",
    "            if end_point in self.end_points:\n",
    "                x = self._modules[end_point](x)\n",
    "        return self.avg_pool(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Resnet Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3x3(in_planes, out_planes, stride=1):\n",
    "    # 3x3x3 convolution with padding\n",
    "    return nn.Conv3d(\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=1,\n",
    "        bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_basic_block(x, planes, stride):\n",
    "    out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n",
    "    zero_pads = torch.Tensor(\n",
    "        out.size(0), planes - out.size(1), out.size(2), out.size(3),\n",
    "        out.size(4)).zero_()\n",
    "    if isinstance(out.data, torch.cuda.FloatTensor):\n",
    "        zero_pads = zero_pads.cuda()\n",
    " \n",
    "    out = Variable(torch.cat([out.data, zero_pads], dim=1))\n",
    " \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resnet Basic Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"https://www.researchgate.net/publication/328091629/figure/fig1/AS:678212644401153@1538709903556/Basic-block-diagram-of-ResNet.ppm\" align= \"left\" heigth= 40% width= 40%> </img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    " \n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    " \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    " \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    " \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    " \n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    " \n",
    "        # Bypass step (summary)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    " \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bottleneck Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0235352.g002&type=large\"  width= 40% heigth= 40% align= \"left\"> </img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-a6b593114487>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mBottleneck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mexpansion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplanes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplanes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownsample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBottleneck\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    " \n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.conv2 = nn.Conv3d(\n",
    "            planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.conv3 = nn.Conv3d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm3d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    " \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "         \n",
    "        #first step\n",
    "        out = self.conv1(x)                                 \n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        # second step\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    " \n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    " \n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        \n",
    "        # Bypass step\n",
    "        out += residual\n",
    "        \n",
    "        out = self.relu(out)\n",
    " \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet Global Architecture\n",
    "<img src= \"https://d2l.ai/_images/resnet18.svg\"  width= 40% heigth= 10% align= \"left\"> </img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    " \n",
    "    def __init__(self,\n",
    "                 block,\n",
    "                 layers,\n",
    "                 sample_size,\n",
    "                 sample_duration,\n",
    "                 shortcut_type='B',\n",
    "                 num_classes=400):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(\n",
    "            3,\n",
    "            64,\n",
    "            kernel_size=7,\n",
    "            stride=(1, 2, 2),\n",
    "            padding=(3, 3, 3),\n",
    "            bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], shortcut_type)\n",
    "        self.layer2 = self._make_layer(\n",
    "            block, 128, layers[1], shortcut_type, stride=2)\n",
    "        self.layer3 = self._make_layer(\n",
    "            block, 256, layers[2], shortcut_type, stride=2)\n",
    "        self.layer4 = self._make_layer(\n",
    "            block, 512, layers[3], shortcut_type, stride=2)\n",
    "        # last_duration = int(math.ceil(sample_duration / 16))\n",
    "        # last_size = int(math.ceil(sample_size / 32))\n",
    "        # self.avgpool = nn.AvgPool3d(\n",
    "        #     (last_duration, last_size, last_size), stride=1)\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    " \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                m.weight = nn.init.kaiming_normal(m.weight, mode='fan_out')\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    " \n",
    "    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            if shortcut_type == 'A':\n",
    "                downsample = partial(\n",
    "                    downsample_basic_block,\n",
    "                    planes=planes * block.expansion,\n",
    "                    stride=stride)\n",
    "            else:\n",
    "                downsample = nn.Sequential(\n",
    "                    nn.Conv3d(\n",
    "                        self.inplanes,\n",
    "                        planes * block.expansion,\n",
    "                        kernel_size=1,\n",
    "                        stride=stride,\n",
    "                        bias=False), nn.BatchNorm3d(planes * block.expansion))\n",
    " \n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    " \n",
    "        return nn.Sequential(*layers)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    " \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    " \n",
    "        x = self.avgpool(x)\n",
    " \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    " \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fine_tuning_parameters(model, ft_begin_index):\n",
    "    \n",
    "    \"Get the tuning parameters of the model (sometimes called 'hyperparameters') and add to them the learning rate (lr)\"\n",
    "    if ft_begin_index == 0:\n",
    "        return model.parameters()\n",
    " \n",
    "    ft_module_names = []\n",
    "    for i in range(ft_begin_index, 5):\n",
    "        ft_module_names.append('layer{}'.format(i))\n",
    "    ft_module_names.append('fc')\n",
    " \n",
    "    parameters = []\n",
    "    for k, v in model.named_parameters():\n",
    "        for ft_module in ft_module_names:\n",
    "            if ft_module in k:\n",
    "                parameters.append({'params': v})\n",
    "                break\n",
    "        else:\n",
    "            parameters.append({'params': v, 'lr': 0.0})\n",
    " \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet18(**kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    return model\n",
    "\n",
    "def resnet34(**kwargs):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFlipWrapper(nn.Module):\n",
    "    \n",
    "    def __init__(self, model, flip_dim=(-1,)):\n",
    "        super(HFlipWrapper, self).__init__()\n",
    "        self.model = model\n",
    "        self.flip_dim = flip_dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            xf = torch.flip(x, self.flip_dim)\n",
    "        pred = self.model(torch.stack([x, xf]))\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, let's build our 3D Model, following the \"PRETRAITENED_MODELS_3D\" and using cuda function for the parallelism "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_3d = []\n",
    "\n",
    "# PRETRAINED_MODELS_3D suit : 'i3d', 'res34','mc3_112', 'mc3_224', 'r2p1_112', 'i3d', 'r2p1_112'\n",
    "    \n",
    "for modeldict in PRETRAINED_MODELS_3D:\n",
    "    if modeldict['type'] == 'i3d':\n",
    "        model = InceptionI3d(157, in_channels=3, output_method='avg_pool')\n",
    "        model.replace_logits(2)\n",
    "        model = model.cuda()\n",
    "        model.eval()\n",
    "        model.load_state_dict(torch.load(modeldict['path']))\n",
    "        models_3d.append({'norm':'i3d', 'model':model})\n",
    "        \n",
    "    elif modeldict['type'] == 'res18':\n",
    "        model = resnet18(num_classes=2, shortcut_type='A', sample_size=224, sample_duration=32)  # , last_fc=True)\n",
    "        model.load_state_dict(torch.load(modeldict['path']))\n",
    "        model = model.cuda()\n",
    "        model.eval()\n",
    "        models_3d.append({'norm':'nil', 'model':model})\n",
    "        \n",
    "    elif modeldict['type'] == 'res34':\n",
    "        model = resnet34(num_classes=2, shortcut_type='A', sample_size=224, sample_duration=32)  # , last_fc=True)\n",
    "        model.load_state_dict(torch.load(modeldict['path']))\n",
    "        model = model.cuda()\n",
    "        model.eval()\n",
    "        models_3d.append({'norm':'nil', 'model':model})\n",
    "        \n",
    "    elif modeldict['type'] == 'mc3_112':\n",
    "        model = mc3_18(num_classes=2)\n",
    "        model.load_state_dict(torch.load(modeldict['path']))\n",
    "        model = model.cuda()\n",
    "        model.eval()\n",
    "        models_3d.append({'norm':'112_imagenet', 'model':model})\n",
    "        \n",
    "    elif modeldict['type'] == 'mc3_224':\n",
    "        model = mc3_18(num_classes=2)\n",
    "        model.load_state_dict(torch.load(modeldict['path']))\n",
    "        model = model.cuda()\n",
    "        model.eval()\n",
    "        models_3d.append({'norm':'224_imagenet', 'model':model})\n",
    "        \n",
    "    elif modeldict['type'] == 'r2p1_112':\n",
    "        model = r2plus1d_18(num_classes=2)\n",
    "        model.load_state_dict(torch.load(modeldict['path']))\n",
    "        model = model.cuda()\n",
    "        model.eval()\n",
    "        models_3d.append({'norm':'112_imagenet', 'model':model})\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type {modeldict['type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys ; sys.path.insert(0, '/kaggle/input/deepfake/deepfake/deepfake/skp/')\n",
    "import yaml\n",
    "\n",
    "with open('/kaggle/input/deepfake/deepfake/deepfake/skp/configs/experiments/experiment001.yaml') as f:\n",
    "    CFG = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "from factory.builder import build_model, build_dataloader\n",
    "\n",
    "CFG['model']['params']['pretrained'] = None\n",
    "model2d = build_model(CFG['model']['name'], CFG['model']['params'])\n",
    "model2d.load_state_dict(torch.load('/kaggle/input/deepfake/SRXT50_094_VM-0.2504.PTH'))\n",
    "model2d = model2d.eval().cuda()\n",
    "loader = build_dataloader(CFG, data_info={'vidfiles': [], 'labels': []}, mode='predict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_and_square_face(video, output_size):\n",
    "    input_size = max(video.shape[1], video.shape[2])  # We will square it, so this is the effective input size\n",
    "    out_video = np.empty((len(video), output_size[0], output_size[1], 3), np.dtype('uint8'))\n",
    "    \n",
    "    for i_frame, frame in enumerate(video):\n",
    "        padded_image = np.zeros((input_size, input_size, 3))\n",
    "        padded_image[0:frame.shape[0], 0:frame.shape[1]] = frame\n",
    "        if (input_size, input_size) != output_size:\n",
    "            frame = cv2.resize(padded_image, (output_size[0], output_size[1])).astype(np.uint8)\n",
    "            frame = padded_image.astype(np.uint8)\n",
    "        out_video[i_frame] = frame\n",
    "    return out_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_crop_video(video, crop_dimensions):\n",
    "    height, width = video.shape[1], video.shape[2]\n",
    "    crop_height, crop_width = crop_dimensions\n",
    "    \n",
    "    y1 = (height - crop_height) // 2\n",
    "    y2 = y1 + crop_height\n",
    "    x1 = (width - crop_width) // 2\n",
    "    x2 = x1 + crop_width\n",
    "        \n",
    "    video_out = np.zeros((len(video), crop_height, crop_width, 3))\n",
    "    for i_frame,frame in enumerate(video):\n",
    "        video_out[i_frame] = frame[y1:y2, x1:x2]\n",
    "        \n",
    "    return video_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_frame_needed_across_faces(faces):\n",
    "    last_frame = 0\n",
    "    \n",
    "    for face in faces:\n",
    "        (frame_from, frame_to), (row_from, row_to), (col_from, col_to) = face\n",
    "        last_frame = max(frame_to, last_frame)\n",
    "        \n",
    "    return last_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms and handle all transformations regrading bounding boxes\n",
    "\n",
    "test_transforms_114_imagenet = A.Compose([A.Resize(height=112, width=112),\n",
    "                                 A.Normalize()])\n",
    "\n",
    "test_transforms_224_imagenet = A.Compose([A.Normalize()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tqdm prints a dynamically updating progressbar every time a value is requested in the loop\n",
    "\n",
    "for videopath, faces in tqdm(faces_by_videopath.items(), total=len(faces_by_videopath)): \n",
    "    try:\n",
    "\n",
    "        if len(faces):\n",
    "            # get the index of the last frame containing a face and load the video frame by frame up to this index\n",
    "            \n",
    "            last_frame_needed = get_last_frame_needed_across_faces(faces)  \n",
    "            video, rescale = load_video(videopath, every_n_frames=1, to_rgb=True, rescale=None, inc_pil=False, max_frames=last_frame_needed)\n",
    "\n",
    "        else:\n",
    "            print(f\"Skipping {videopath} as no faces found\")\n",
    "            continue\n",
    "            \n",
    "        for modeldict in models_3d:\n",
    "            preds_video = []\n",
    "            model = modeldict['model']\n",
    "            \n",
    "            model = HFlipWrapper(model=model)\n",
    "\n",
    "            for i_face, face in enumerate(faces):\n",
    "                (frame_from, frame_to), (row_from, row_to), (col_from, col_to) = face\n",
    "                \n",
    "                # Extract from the video, the batch of rois corresponding to the face, resize and square it\n",
    "                x = video[frame_from:frame_to, row_from:row_to + 1, col_from:col_to + 1]\n",
    "                x = resize_and_square_face(x, output_size=OUTPUT_FACE_SIZE)\n",
    "\n",
    "                if PRE_INFERENCE_CROP and PRE_INFERENCE_CROP != OUTPUT_FACE_SIZE:\n",
    "                # Center x on a blank (white) video and affect the video to x\n",
    "                    x = center_crop_video(x, PRE_INFERENCE_CROP)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    \n",
    "                    # Scaling x depending on the model\n",
    "                    \n",
    "                    if modeldict['norm'] == '112_imagenet':\n",
    "                        x = np.array([test_transforms_114_imagenet(image=frame)['image'] for frame in x])\n",
    "                    elif modeldict['norm'] == '224_imagenet':\n",
    "                        x = np.array([test_transforms_224_imagenet(image=frame)['image'] for frame in x])\n",
    "                        \n",
    "                    x = torch.from_numpy(x.transpose([3, 0, 1, 2])).float()\n",
    "                    \n",
    "                    if modeldict['norm'] == 'i3d':\n",
    "                        x = (x / 255.) * 2 - 1\n",
    "                    elif modeldict['norm'] == 'nil':\n",
    "                        pass\n",
    "                    elif modeldict['norm'] == '112_imagenet':\n",
    "                        pass\n",
    "                    elif modeldict['norm'] == '224_imagenet':\n",
    "                        pass\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unknown normalisation mode {modeldict['norm']}\")\n",
    "\n",
    "                    y_pred = model(x.cuda())\n",
    "                    prob0, prob1 = torch.mean(torch.exp(F.log_softmax(y_pred, dim=1)),dim=0)\n",
    "                    if REVERSE_PROBS:\n",
    "                        prob1 = 1-prob1\n",
    "                    preds_video.append(float(prob1))\n",
    "                    \n",
    "            videoname = os.path.basename(videopath)\n",
    "            if preds_video:\n",
    "                predictions[videoname].extend([USE_FACE_FUNCTION(preds_video)] * RATIO_3D)\n",
    "            \n",
    "        try:\n",
    "            FRAMES2D = 32\n",
    "            # Ian's 2D model\n",
    "            coords = coords_by_videopath[videopath]\n",
    "            videoname = os.path.basename(videopath)\n",
    "            preds_video = []\n",
    "            for i_coord, coordinate in enumerate(coords):\n",
    "                (frame_from, frame_to), (row_from, row_to), (col_from, col_to) = faces[i_coord]\n",
    "                x = []\n",
    "                for coord_ind, frame_number in enumerate(range(frame_from, min(frame_from+FRAMES2D, frame_to-1))):\n",
    "                    if coord_ind >= len(coordinate):\n",
    "                        break\n",
    "                    x1, y1, x2, y2 = coordinate[coord_ind]\n",
    "                    x.append(video[frame_number, y1:y2, x1:x2])\n",
    "                x = np.asarray(x)\n",
    "                # Reverse back to BGR because it will get reversed to RGB when preprocessed\n",
    "                #x = x[...,::-1]\n",
    "                # Preprocess\n",
    "                x = loader.dataset.process_video(x)\n",
    "                #x = np.asarray([loader.dataset.process_image(_) for _ in x])\n",
    "                # Flip every other frame\n",
    "                x[:,::2] = x[:,::2,:,::-1]\n",
    "                # RGB reverse every 3rd frame\n",
    "                #x[:,::3] = x[::-1,::3]\n",
    "                with torch.no_grad():\n",
    "                    out = model2d(torch.from_numpy(np.ascontiguousarray(x)).unsqueeze(0).cuda())\n",
    "                #out = np.median(out.cpu().numpy())\n",
    "                preds_video.append(out.cpu().numpy())\n",
    "            if len(preds_video) > 0:\n",
    "                predictions[videoname].extend([USE_FACE_FUNCTION(preds_video)] * RATIO_2D)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Video {videopath}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_show = x.transpose(1,2,3,0)\n",
    "x_show -= np.min(x_show)\n",
    "x_show /= np.max(x_show)\n",
    "def view(x, nrows=4, ncols=4):\n",
    "    indices = np.linspace(0, len(x)-1, nrows*ncols) \n",
    "    for ind, i in enumerate(indices):\n",
    "        plt.subplot(nrows,ncols,ind+1)\n",
    "        plt.imshow(x[int(i),...,::-1])\n",
    "    plt.show()\n",
    "\n",
    "view(x_show, 5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in predictions.items():\n",
    "    string = '{} : {:.4f} //'.format(k, np.mean(v))\n",
    "    for proba in v:\n",
    "        string += ' {:.4f}'.format(proba)\n",
    "    print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "for modeldict in models_3d:\n",
    "    del modeldict['model']\n",
    "    del modeldict\n",
    "del models_3d\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill in missing predictions with 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for videopath in videopaths:\n",
    "    videoname = os.path.basename(videopath)\n",
    "    if videoname not in predictions:\n",
    "        predictions[videoname] = [DEFAULT_MISSING_PRED]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate ensembled prediction & clamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_ensembled = {}\n",
    "\n",
    "for videopath, pred in predictions.items():\n",
    "    #print(f\"{videopath} Got {len(pred)} predictions\")\n",
    "    preds_ensembled[videopath] = np.clip(np.mean(pred), PROB_MIN, PROB_MAX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /kaggle/working/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(preds_ensembled)} predictions\")\n",
    "with open(\"submission.csv\", 'w') as f:\n",
    "    f.write(\"filename,label\\n\")\n",
    "    for filename, label in preds_ensembled.items():\n",
    "        f.write(f\"{filename},{label}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -10 submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1\n",
    "def function(a):\n",
    "    if a>5 : return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "function(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "02efc37894e64b709d14578e4b8ab27b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "11e7ee50e2d04eba89d682ab2ba5211b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "1a60c964c47e4204bf0fdbfeabe79f5a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1ba9da740b4346b082d5f3923fbcdbac": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3942b1c4344c4d75a1ee3a39a914febe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "3ce1a5405ee74d4b96334e6cb3d0d45e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_44710822d5f34b67908ff8f70d6666db",
       "placeholder": "​",
       "style": "IPY_MODEL_ddc5b317b94145cead99d1f8f1078d68",
       "value": " 400/400 [39:01&lt;00:00,  5.85s/it]"
      }
     },
     "44710822d5f34b67908ff8f70d6666db": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "54073797e87e4dd4b4c43a0e7abc272a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_92732868e9d34847a8f4b50defc5b427",
        "IPY_MODEL_cfee26e840fa4f65af2604eaab1c06e7"
       ],
       "layout": "IPY_MODEL_e4eb8292afc24f04aa316620b495547b"
      }
     },
     "60db0e4beea54d04b6cc5c8ed86e8a64": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "65ce22e6465d4561a9a583bc933914ec": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6de36b4cb6334731be6d1ea6f02307e8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_65ce22e6465d4561a9a583bc933914ec",
       "max": 400,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a967b9b38bb64beb9cc7b18d79ecf649",
       "value": 400
      }
     },
     "72007f7545df441fa9302834c0d27ca8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6de36b4cb6334731be6d1ea6f02307e8",
        "IPY_MODEL_3ce1a5405ee74d4b96334e6cb3d0d45e"
       ],
       "layout": "IPY_MODEL_a2bb4de871354b46a9cf48cbd0e536f2"
      }
     },
     "7497f7b3d68340bdbba5b6da4bbf2b64": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "815eb24aae0d4cbdac5a5d1d898d0277": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "92732868e9d34847a8f4b50defc5b427": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1a60c964c47e4204bf0fdbfeabe79f5a",
       "max": 400,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7497f7b3d68340bdbba5b6da4bbf2b64",
       "value": 400
      }
     },
     "9579a59b733f4a6faaa8b76b898cceda": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1ba9da740b4346b082d5f3923fbcdbac",
       "placeholder": "​",
       "style": "IPY_MODEL_815eb24aae0d4cbdac5a5d1d898d0277",
       "value": " 5/5 [00:06&lt;00:00,  1.34s/it]"
      }
     },
     "a2bb4de871354b46a9cf48cbd0e536f2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a967b9b38bb64beb9cc7b18d79ecf649": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "b4167b61bef44d6087c70db2ad24b0f4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_60db0e4beea54d04b6cc5c8ed86e8a64",
       "max": 5,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3942b1c4344c4d75a1ee3a39a914febe",
       "value": 5
      }
     },
     "cfee26e840fa4f65af2604eaab1c06e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_fbc1888a2411470a9e1662ac2fa46873",
       "placeholder": "​",
       "style": "IPY_MODEL_11e7ee50e2d04eba89d682ab2ba5211b",
       "value": " 400/400 [06:26&lt;00:00,  1.04it/s]"
      }
     },
     "ddc5b317b94145cead99d1f8f1078d68": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e4eb8292afc24f04aa316620b495547b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f06e14f35ada4376a2feef853a71e9da": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b4167b61bef44d6087c70db2ad24b0f4",
        "IPY_MODEL_9579a59b733f4a6faaa8b76b898cceda"
       ],
       "layout": "IPY_MODEL_02efc37894e64b709d14578e4b8ab27b"
      }
     },
     "fbc1888a2411470a9e1662ac2fa46873": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
